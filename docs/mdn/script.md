# mdn 発表 台本

[PDF 版](https://tiny.cc/lapsq-sh/docs/5_mdn/test/script/0.pdf?web=1)

## タイトル

(スズキ)  
今から発表をはじめます。

## 1. はじめに

★  
現在は情報技術の進歩により、街には多くのカメラが設置されています。
しかし、長時間の動画から必要な部分を確認するためには多くの時間が必要なため、実用的ではありません。
そこで、必要な場面のみを抽出するシステムがあれば、データ容量を減らすことができ、様々な場面で活用できるのではないかと考えました。

★  
まずは身近な動物の動画を題材とし、動物園や子どもたちの学習に活用できるようなシステムを開発することにしました。
私たちは AI による動画短縮のための場面抽出分類システム開発の研究を行っています。
まず、研究の背景です。

★  
動物園に行った際、目当ての動物がずっと寝ていたり、座ったりしていて、悲しい思いをしたことがある人は多いと思います。
そのため、動物を撮影した長時間の動画を解析し、動物が動いている部分のみを抽出しようと考えました。

★  
本研究で開発したシステムはタイムラプスを 圧縮する つまり squash するという意味で、lapsquash という名称に決定しました。
これが、システムの全体図です。

## 2. 論理・研究手法

★

<!-- TODO: なにか -->

本研究の流れを順に説明します。

★ (1)  
(マチダ)  
まず、動物行動を捉えるために、Raspberry Pi とカメラモジュールを用いて動物の動きを撮影します。

★  
リアルタイムで動画を細分化する方法として、類似度ハッシュをトリガーとすることで、必要な動きのある場面のみを抽出できます。

★ (2)  
切り抜き範囲を決めるアルゴリズムは 1 フレームごとに算出される類似度ハッシュの変化を求めるために、

★  
過去 1 秒の類似度ハッシュの値を最小二乗法で近似し、その傾きを取ります。

★  
過去のフレームをもとに指数移動平均をとって動的な切り抜きを始めるしきい値を決めました。

★ (3)  
続いて、動画のタイル化と圧縮を行いました。AI のインプット用にビデオクリップから時間ごとのタイル画像を生成します。これにより、画像入力ができる Bard に解析できるようになりました。

★ (4)  
その後、ストレージを操作する API を使って、細分化したデータをオブジェクトストレージである SharePoint に保存しました。

★ (5)  
その後、行動分析をするために、Bard の Image Input API を使用して、タイル画像の特徴と説明、事前に私たちで決めたタグ付けを AI にしてもらいます。これらの結果は先ほどのデータに再格納することで、Web アプリで見られるようにしました。

★ (6)  
(オサカダ)  
最後に、行動の解析結果とダイジェストを見るためのアプリを制作しました。
Viewer では切り抜いた動画を AI が生成した説明やタグと一緒に見ることができます。
デザインについてです。

★
ニューモフィズムと Material Design を一部採用し、操作しやすくするために、シンプルな UI にすることでパソコン操作が苦手な人でも直感的な操作ができるようになっています。

★  
これは実際のアプリの動作です。
プロジェクトを開くと、閲覧画面に移行し、先ほど説明したように、タグと説明と右側に切り抜いた他の動画を選択して閲覧することができます。

## 3. 研究結果・検証

★  
開発した切り抜きアルゴリズムの精度を検証するために対象の動画に対して人の手で切り抜いた範囲データと Squasher Core が切り抜いた範囲データを比較し、両者の一致率を算出しました。

★  
その結果、定点カメラにより撮影されたニホンザルの 30 分間の映像の総時間は切り抜き前より 47% 短くなり、一致率は 43.5 % でした。
また、ヒツジの 30 分間の映像の総時間は切り抜き前より 64% 短くなり、一致率は 54.4 % でした。

★  
10 月 22 日 東山動物園にご協力いただき、4 名の職員の方にシステムのプレゼンテーションを行いました。
飼育員の方のご意見では、今回使用した分類の方法では曖昧な点が多く、動物行動の定義付けに関する難しさをご指摘いただきました。
そのため、汎用的なシステムよりも、特定の動物に特化した行動分析を行うシステムの方が有用であること、映像に加えて音声を解析することで、精度を高めることにつながるとのご意見も頂きました。
今回、飼育員の方の活用だけでなく、子どもの学習の活用も目指していましたが、これに対しても必要な情報が異なるため、飼育員用と来園者用でシステムを分ける必要があると分かりました。

## 4. 結論

(マチダ)  
★  
結論は、本研究のシステムは飼育員用と来園者用の２つの立場の方に使ってもらえるように設計しましたが、共用にするのは難しく、動物の行動分析の定義も難しいことがわかりました。
しかし、動画のハッシュ値から行動している部分を抽出することで長時間の動画から行動している場面のみを見ることが可能となりました。

★  
今後の課題は、切り抜き範囲が細かすぎることや、AI による説明の精度の向上です。
例としては、“青いコンテナの上にいるサル” を “水槽の中で泳ぐサル” と説明してしまいました。
そのままの AI を利用するには精度の限界があるため、今後は特定の動物の行動を学習させたモデルを使用しようと考えています。

## おわりに

★  
これで発表を終わります。ありがとうございました。
