#import "assets/modules/_conf.typ": *
#import "assets/modules/articleFig.typ": articleFig
#import "assets/modules/personHandled.typ": personHandled

#let member = json("assets/member.json")

#show: doc => conf(
  title: (
    // 表題 - 和文
    jp: "AI による動画短縮のための場面抽出分類システム開発",
    // 表題 - 英文
    en: "Development of Scene Extraction and Classification System for Video Shortening Using AI",
  ),
  // メンバー
  member: member.students,
  // 指導教諭
  teacher: member.teachers.join(", "),
  // 発表学科略号/発表番号
  header: "J06",
  doc,
)

= はじめに
#i 情報技術の進歩により, 街には多くのカメラが設置されている. しかし, 長時間の動画から必要な部分を確認するためには多くの時間を要する。
そこで、必要な場面のみを抽出するシステムがあれば、様々な場面で活用できるのではないかと考えた。また、必要な動画のみを保存することでデータ容量を減らすことができる。
まずは身近な動物の動画を題材とし、動物園や子どもたちの学習に活用できるようなシステムを開発することにした。

#i 先行研究では、MPEG 動画から画像を複合せずにマクロブロック (映像の処理単位) から動画のカットを試みている。 しかし、この方法では、MPEG
動画のみにしか対応ができない。私たちは、MPEG 動画だけでなく、カメラ映像からリアルタイムに解析、抽出することによって動画を短縮し、AI
を用いて分類するシステムを開発する。また、誰でも簡単に活用できるよう、Web のアプリケーションとする。


= 論理・実験
== システム概要

#articleFig(
  imgPath: "../assets/img/overall_view.png",
  imgWidth: 82mm,
  imgCaption: [システム全体図],
)[
  #i まず、動物の動きを Raspberry Pi
  のカメラモジュールで撮影し、細分化する。ビデオクリップは、時間のメタデータとオブジェクトストレージに保存する。
  次に、保存されたデータから、ビデオクリップごとにタイル状に並べた画像を生成する。タイル画像を Bard の Image Input API
  で分析、動物行動に分類し、メタデータに書き込む。 最後に、分類し短縮されたビデオクリップを Web アプリ上に表示する (図 1)。
]

== ??
=== 動物行動
既存の論文を参考にし、動物行動を8 つに分類した (表 1).
#figure(align(center)[#table(
    columns: 2,
    align: (col, row) => (auto, auto,).at(col),
    inset: 6pt,
    [タグ],
    [行動],
    [$F$],
    [採食],
    [$S$],
    [座る],
    [$G$],
    [毛づくろい],
    [$S l$],
    [睡眠],
    [$Q$],
    [喧嘩],
    [$M$],
    [動く、跳ぶ、走る、歩く],
    [$P$],
    [遊ぶ],
    [$C$],
    [交尾],
  )])

しかし、長時間の動画をそのまま AI で分類することは、データ量が大きく難しい。そのため、まずは、必要な動きのある場面のみを抽出することにした。
動物行動の変化の境目は、類似度ハッシュを利用して検出した。類似度ハッシュは、画像の特徴を抽出してハッシュ値に変換する関数である。
隣り合う画像のハッシュ値の差によって、画像の変化とみなせる (図 2)。

1 フレームごとに算出された類似度ハッシュの変化を得るために、過去 1 秒の類似度ハッシュ値を最小二乗法の 1 次式で近似し、その傾きを算出した。
開発当初は、静的なしきい値で切り抜きを行っていたが、動画によって最適なしきい値が異なるため、過去の傾きから指数移動平均を算出し、動的なしきい値を使用した切り抜きを行う方法に変更した。
プログラムの処理内容を図 3 に示す。

レンダラーとモデルが事前に定義したインターフェースを継承して、切り抜きを行うプログラムを作成した。レンダラー抽象クラスは、 初期化処理と画面描画、1
フレームごとの画面更新をするメソッドを持つ。また、モデル抽象クラスは、初期化処理と 1 フレームごとの更新処理と終了処理を行うメソッドを持つ。
モデルの定義はモデル抽象クラスを継承することで、モデルを拡張機能のように扱えるため、分離性が高まった。これにより複雑な処理を内容ごとに分離し、パッケージ化できた。

また、イベントループ抽象クラスのコンストラクタにシングルトンストアを注入することで、継承しているレンダラーとモデル内で簡単にシングルトンストアを利用可能になり、モデル内で安全にステートの読み書きができるようになった。
これらの設計の工夫によって、後続のモデルにデータを渡しやすくなり、リアルタイム性に優れた OpenCV や PyQtGraph、PySide
などの適切なフレームワークを組み合わせて、データの可視化も行った (図 4)。

=== 動画のタイル化
AI のインプット用にビデオクリップからタイル画像を生成する。これにより、Bard
で解析可能になる。解析が終了したタイル画像のメタデータに、ビデオクリップごとの特徴と説明とタグのデータを再格納する。
これらのファイルをオブジェクトストレージである SharePoint に保存する。SharePoint は、Graph API
経由でアクセスが可能である。OAuth 2.0 に準拠するように実装し、 ユーザーが squasher や viewer
にアクセス権限の委任をすることで、ネットワーク上で安全にデータを保存できる。これによりタグ付けされたビデオクリップを Web
のアプリケーションで閲覧できるようになった。

=== 閲覧 Web アプリケーション (Viewer)
オブジェクトストレージからファイルを読み込み、AI が生成した説明やタグをわかりやすく表示した。Material Design
を一部採用し、ニューモフィズムと丸みを持たせたデザインにすることによって操作しやすく、
親しみやすいアプリにした。ニューモフィズムとはフラットデザインに凹凸を加えたデザインのことである。また、インタラクティブな UI
にすることでパソコン操作が苦手な人でも直感的な操作が可能になっている。
デバイスの画面サイズに合わせてデザインを最適化しやすいように装飾を減らし、シンプルなデザインを採用した。配色にも Material Design の Color
System を採用することでコンテンツ数が増えた際でも視認性を保つことができた。


= 実験結果
Squasher Core の切り抜きアルゴリズムの精度を検証するために、動画に対して `(始点 [秒], 終点 [秒])` の範囲データを定めた。
人の手で切り抜いた範囲データと Squasher Core が切り抜いた範囲データを比較し、両者の一致率を算出した。また、許容される誤差の範囲を 1 秒とした。
その結果、定点カメラにより撮影されたニホンザルの 30 分間の動画 (おさるランド＆アニタウン) の精度は 43.5 %、ヒツジの 30 分間の動画
(石狩ひつじ牧場) の精度は、54.4 % となった。

=== AI によるタグ付けの精度検証
タグ付けについては、未検証である。その理由は、様々な行動の含まれた動画が無く、検証が難しかったためである。

=== 動物園での意見交換
2023 年 10 月 22 日 (日) 東山動物園にご協力いただき、飼育を担当されている 3 名の方にシステムのプレゼンテーションを行った。
飼育員の方のご意見では、今回使用した分類の方法ではあいまいな点が多く、動物行動の定義付けに関する難しさをご指摘いただいた。そのため、汎用的なシステムよりも、
特定の動物に特化した行動分析を行うシステムの方が有用だとわかった。また、映像に加えて、音声を解析することで、精度を高めることにつながるとのご意見も頂いた。
今回、飼育員の方の活用だけでなく、子どもの学習の活用も目指したが、これに対しても、必要な情報が異なるため、システムを分ける必要があるとわかった。

= 比較・考察
動物行動の変化の境目を検出するためのトリガーとして類似度ハッシュを採用したが、切り抜きアルゴリズムとしての想定した精度を発揮できなかった。
そこで、最新の物体検出モデル YOLO v8
とのハイブリッド方式で行うのがよいと考えた。類似度ハッシュは画像の背景のみが変化した場合でも変化してしまう。それに対して、最新の物体検出モデル YOLO v8
を使用することで、 類似度ハッシュと合わせて観察対象のみの動きを捉えることができた。しかし、実際に作成し、動作を確認したところ YOLO v8
では処理が遅く動画の書き出しに影響が出たため、類似度ハッシュのみをトリガーとしたが、 今後精度の向上には YOLO v8
よりも処理の速い物体検出の仕組みを追加すべきである。

ビデオクリップの長さが 3–4 秒となった結果、AI が動物の行動を解析するのに十分な量の情報が取得できず、当初予想していたようなタグ付けができなかった。
今後は、ビデオクリップの適切な長さを研究し改善する。また、AI
が生成した説明は、青いコンテナの上で歩いているサルの動画を「水槽の中にいるサル」といった説明であった。 そのままの生成 AI
を利用するには精度の限界があるため、今後は特定の動物の行動を学習させたモデルを使用する。

= 結論
#i あのイーハトーヴォのすきとおった風、夏でも底に冷たさをもつ青いそら、うつくしい森で飾られたモリーオ市、郊外のぎらぎらひかる草の波。

= 今後の課題
情報技術の進歩は日進月歩であり、長時間の映像から必要な部分を効率的に抽出するシステムの需要は今後、高まるはずである。この需要に応えるため、本研究ではそのシステムの開発を試みた。
その結果、人の手による切り抜きデータと Squasher Core の切り抜きアルゴリズムの比較による精度検証では 40–50 %
という結果が得られた。また、AI によるタグ付けの精度は未検証であり、今後の調査が必要である。
さらに、東山動物園の飼育員との意見交換会では、特定の動物に特化した行動分析と音声解析の導入の提案があり、システムを適切な大きさで分割する必要があるとわかった。

今後の課題は、動画の切り出しのアルゴリズムとして、画像の類似度ハッシュと物体検出を組み合わせ、十分な解析速度を有するシステムの開発を目指す。 AI
がタグ付けを行う際に必要十分なデータとして、複数のカメラ映像、音声データを加え、解析の質を向上させ、タグ付けの精度を高める研究をする。

= 引用文献
#bibliography("refs.yml", title: [])

#pagebreak()

#let handed = (
  //
  "machida": (
    // 自分が担当したところ
    handled: [
      - Squasher, Analyzer の 設計, 実装
      - 研究環境とソースリポジトリの管理
    ],
    // 自分が担当したところの結果
    handledResult: [
      - Squasher
        - Core\
          映像の読み込みと類似度ハッシュと算出, 映像の切り抜きを行い, 算出された値のグラフをリアルタイムに描画するソフトウェア
        - CLI\
          SharePoint にビデオクリップをアップロードするための OAuth 2.0 に準拠したコマンドラインツール
        - Inspector\
          映像の切り抜き精度とタグ付け精度の検証と類似度ハッシュの種類による精度の変化の対照実験するためのツール
      - Analyzer\
        SharePoint との接続とラッパー API の実装, Bard のプロンプトの考案
    ],
    // 自分が担当したところの今後の課題・考察
    futureIssues: [
      - 切り抜きアルゴリズムの改善\
        #i 類似度ハッシュの変化率を推移を, 過去の変化率をもとに指数移動平均を求めているが, 切り抜き範囲が短すぎるという課題があった.\
        #i 切り抜きをするかどうかの動的なしきい値を遅延評価をによって算出し, あとからグループ化することで適切な粒度で切り抜き範囲を決定することができると考察する.
    ],
  ),
  //
  "osakada": (
    // 自分が担当したところ
    handled: [
      - Webアプリケーションの制作
      - WebアプリケーションのUI設計
      - ポスターの制作
    ],
    // 自分が担当したところの結果
    handledResult: [
      - Webアプリケーションを作成することで撮影された動画や、AIでの説明を簡単に閲覧することが可能となった。
    ],
    // 自分が担当したところの今後の課題・考察
    futureIssues: [
      - 検索機能、タグごとでのフィルター機能などのUX向上
      - レスポンシブに対応したUIの設計
      - 動画再生の補助機能
    ],
  ),
  "suzuki": (
    // 自分が担当したところ
    handled: [
      + XXX
    ],
    // 自分が担当したところの結果
    handledResult: [
      + XXX
      + XXX
    ],
    // 自分が担当したところの今後の課題・考察
    futureIssues: [
      + XXX
    ],
  ),
)

#for p in member.students {
  let hd = handed.at(p.id)
  personHandled(
    //
    person: p,
    handled: hd.handled,
    handledResult: hd.handledResult,
    futureIssues: hd.futureIssues,
  )
  if p.name != member.students.last().name {
    hr
  }
}