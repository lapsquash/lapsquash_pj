#import "assets/modules/_conf.typ": *
#import "assets/modules/articleFig.typ": articleFig
#import "assets/modules/personHandled.typ": personHandled

#let member = json("assets/member.json")

#show: doc => conf(
  title: (
    // 表題 - 和文
    jp: "AI による動画短縮のための場面抽出分類システム開発",
    // 表題 - 英文
    en: "Development of Scene Extraction and Classification System for Video Shortening Using AI",
  ),
  // メンバー
  member: member.students,
  // 指導教諭
  teacher: member.teachers.join(", "),
  // 発表学科略号/発表番号
  header: "J06",
  doc,
)

= はじめに
#i 情報技術の進歩により, 街には多くのカメラが設置されている. しかし, 長時間の動画から必要な部分を確認するためには多くの時間を要する. そこで,
必要な場面のみを抽出するシステムがあれば, 様々な場面で活用できるのではないかと考えた. また, 必要な動画のみを保存することでデータ容量を減らすことができる.
まずは身近な動物の動画を題材とし, 動物園や子どもたちの学習に活用できるようなシステムを開発することにした.

#i 先行研究 @MPEG動画像カット検出法 では, MPEG 動画から画像を複合せずにマクロブロック (映像の処理単位) から動画のカットを試みている.
しかし この方法では, MPEG 動画のみにしか対応ができない. 私たちは, MPEG 動画だけでなく, カメラ映像からリアルタイムに 解析, 抽出
することによって動画を短縮し, AI を用いて分類するシステムを開発する. また, 誰でも簡単に活用できるよう Web のアプリケーションとする.

= 論理・実験
== 概要

#articleFig[
  #i まず, 動物の動きを Raspberry Pi のカメラモジュールで撮影し, 細分化する. ビデオクリップは,
  時間のメタデータとオブジェクトストレージに保存する. 次に, 保存されたデータから, ビデオクリップごとにタイル状に並べた画像を生成する. タイル画像を
  Bard の Image Input API で分析, 動物行動に分類し, メタデータに書き込む. 最後に, 分類し短縮されたビデオクリップを Web
  アプリ上に表示する (@overall_view).
][
  #figure(
    image("assets/img/overall_view.png", width: 80mm),
    caption: [システム概要],
    supplement: "図",
  )<overall_view>
]
#pagebreak()
== 動物行動

#grid(
  columns: 2,
  gutter: 6pt,
  [
    #i 既存の論文 @ニホンザル野生群の日周活動リズム-予報 @ニホンザルが毛づくろいをする理由 @ニホンザルの基礎生態と特定計画策定・運用のポイントについて
    @ニホンザルホームページ @ニホンザルの一日 @チンパンジーとヒトの共通点・相違点 @ニホンザルの生態と生息環境について @おさるランド @ひつじのライブカメラ
    @動物園動物観察アプリケーションの開発 を参考にし, 動物行動を 8 つに分類した (@action_table). しかし, 長時間の動画をそのまま AI
    で分類することは, データ量が大きく難しい. そのためまずは, 必要な動きのある場面のみを抽出することにした.

    #i 動物行動の変化の境目は, 類似度ハッシュを利用して検出した. 類似度ハッシュは, 画像の特徴を抽出してハッシュ値に変換する関数である.
    隣り合う画像のハッシュ値の差によって, 画像の変化とみなせる (@p_hash).

    #i その後, 1 フレームごとに算出された類似度ハッシュの変化を得るために, 過去 1 秒の類似度ハッシュ値を最小二乗法の 1 次式で近似し,
    その傾きを算出した.
  ],
  [#figure(table(
      columns: 2,
      fill: (_, row) => if row == 0 { luma(230) } else { white },
      inset: 6pt,
      [タグ],
      [行動],
      //
      [$F$],
      [採食],
      [$S$],
      [座る],
      [$G$],
      [毛づくろい],
      [$S l$],
      [睡眠],
      [$Q$],
      [喧嘩],
      [$M$],
      [動く, 跳ぶ, 走る, 歩く],
      [$P$],
      [遊ぶ],
      [$C$],
      [交尾],
    ), caption: [動物行動の分類], supplement: "表")<action_table>
  ],
)

#i 開発当初は, 静的なしきい値で切り抜きを行っていたが, 動画によって最適なしきい値が異なるため, 過去の傾きから指数移動平均を算出し,
動的なしきい値を使用した切り抜きを行う方法に変更した. プログラムの処理内容を @squasher_core_handle に示す.

#i レンダラーとモデルが事前に定義したインターフェースを継承して, 切り抜きを行うプログラムを作成した. レンダラー抽象クラスは, 初期化処理と画面描画, 1
フレームごとの画面更新をするメソッドを持つ. また, モデル抽象クラスは, 初期化処理と 1 フレームごとの更新処理と終了処理を行うメソッドを持つ.
モデルの定義はモデル抽象クラスを継承することで, モデルを拡張機能のように扱えるため, 分離性が高まった. これにより複雑な処理を内容ごとに分離し,
パッケージ化できた.

#table(
  columns: 2,
  align: center + bottom,
  stroke: 0pt,
  [#figure(
      image("assets/img/p_hash.png", width: 100%),
      caption: "類似度ハッシュの変化率を用いた行動変化の境界検出",
      supplement: "図",
    )<p_hash>],
  //
  [#figure(
      image("assets/img/squasher_core_spec.png", width: 100%),
      caption: "Squasher Core のフレーム毎の処理",
      supplement: "図",
    )<squasher_core_handle>],
)

#i また, イベントループ抽象クラスのコンストラクタにシングルトンストアを注入することで,
継承しているレンダラーとモデル内で簡単にシングルトンストアを利用可能になり, モデル内で安全にステートの読み書きができるようになった.
#i これらの設計の工夫によって, 後続のモデルにデータを渡しやすくなり, リアルタイム性に優れた OpenCV や PyQtGraph, PySide
などの適切なフレームワークを組み合わせて, データの可視化も行った.

== 動画のタイル化
#i AI のインプット用にビデオクリップからタイル画像を生成する. これにより, Bard で解析可能になる. 解析が終了したタイル画像のメタデータに,
ビデオクリップごとの特徴と説明とタグのデータを再格納する. これらのファイルをオブジェクトストレージである SharePoint に保存する.
SharePoint は, Graph API 経由でアクセスが可能である. OAuth 2.0 に準拠するように実装し, ユーザーが Squasher や
Viewer にアクセス権限の委任をすることで, ネットワーク上で安全にデータを保存できる. これによりタグ付けされたビデオクリップを Web
のアプリケーションで閲覧できるようになった.

== 閲覧 Web アプリケーション (Viewer)

#articleFig[

  #i オブジェクトストレージからファイルを読み込み, AI が生成した説明やタグをわかりやすく表示した (@viewer_visual). Material
  Design を一部採用し, ニューモフィズムと丸みを持たせたデザインにすることによって操作しやすく親しみやすいアプリにした.
  ニューモフィズムとはフラットデザインに凹凸を加えたデザインのことである.][
  #figure(
    image("assets/img/viewer_visual.png", width: 82mm),
    caption: [Viewer の画面],
    supplement: "図",
  )<viewer_visual>
]

#i また, インタラクティブな UI にすることでパソコン操作が苦手な人でも直感的な操作が可能になっている.
デバイスの画面サイズに合わせてデザインを最適化しやすいように装飾を減らし, シンプルなデザインを採用した. 配色にも Material Design の
Color System を採用することでコンテンツ数が増えた際でも視認性を保つことができた.

= 実験結果
#i Squasher Core の切り抜きアルゴリズムの精度を検証するために, 動画に対して `(始点 [秒], 終点 [秒])` の範囲データを定めた.
人の手で切り抜いた範囲データと Squasher Core が切り抜いた範囲データを比較し, 両者の一致率を算出した. また, 許容される誤差の範囲を 1
秒とした. その結果, 定点カメラにより撮影されたニホンザルの 30 分間の動画 (おさるランド＆アニタウン) の精度は 43.5 %, ヒツジの 30
分間の動画 (石狩ひつじ牧場) の精度は, 54.4 % となった.

== AI によるタグ付けの精度検証
#i タグ付けについては, 未検証である. その理由は, 様々な行動の含まれた動画が無く, 検証が難しかったためである.

== 動物園での意見交換

#articleFig[
  #i 2023 年 10 月 22 日 (日) 東山動物園にご協力いただき, 飼育を担当されている 3 名の方にシステムのプレゼンテーションを行った
  (@zoo). 飼育員の方のご意見では, 今回使用した分類の方法ではあいまいな点が多く, 動物行動の定義付けに関する難しさをご指摘いただいた. そのため,
  汎用的なシステムよりも, 特定の動物に特化した行動分析を行うシステムの方が有用だとわかった.
][
  #figure(
    image("assets/img/zoo.png", width: 80mm),
    caption: [東山動物園での意見交換],
    supplement: "図",
  )<zoo>
]

#i また, 映像に加えて, 音声を解析することで, 精度を高めることにつながるとのご意見も頂いた. 今回, 飼育員の方の活用だけでなく,
子どもの学習の活用も目指したが, これに対しても, 必要な情報が異なるため, システムを分ける必要があるとわかった.

= 比較・考察
#i 動物行動の変化の境目を検出するためのトリガーとして類似度ハッシュを採用したが, 切り抜きアルゴリズムとしての想定した精度を発揮できなかった. そこで,
最新の物体検出モデル YOLO v8 とのハイブリッド方式で行うのが良いと考えた. 類似度ハッシュは画像の背景のみが変化した場合でも変化してしまう.
それに対して, 最新の物体検出モデル YOLO v8 を使用することで, 類似度ハッシュと合わせて観察対象のみの動きを捉えることができた.

#i しかし, 実際に作成し, 動作を確認したところ YOLO v8 では処理が遅く動画の書き出しに影響が出たため類似度ハッシュのみをトリガーとしたが,
今後精度の向上には YOLO v8 よりも処理の速い物体検出の仕組みを追加すべきである.

#i ビデオクリップの長さが 3--4 秒となった結果, AI が動物の行動を解析するのに十分な量の情報が取得できず,
当初予想していたようなタグ付けができなかった. 今後は, ビデオクリップの適切な長さを研究し改善する. また, AI が生成した説明は,
青いコンテナの上で歩いているサルの動画を「水槽の中にいるサル」といった説明であった. そのままの生成 AI を利用するには精度の限界があるため,
今後は特定の動物の行動を学習させたモデルを使用する.

= 結論
#i 本研究のシステムは飼育員用と来園者用の 2 つの立場の方に使ってもらえるように設計したが, 共用にするのは難しく,
動物の行動分析の定義も難しいことがわかった. しかし, 動画の類似度ハッシュの変化率から行動している部分を抽出することで,
長時間の動画から行動している場面のみを見ることが可能となった.

= 今後の課題
#i 情報技術の進歩は日進月歩であり, 長時間の映像から必要な部分を効率的に抽出するシステムの需要は今後, 高まると考える. この需要に応えるため,
本研究ではそのシステムの開発を試みた. その結果, 人の手による切り抜きデータと Squasher Core の切り抜きアルゴリズムの比較による精度検証では
40--50 % という結果が得られた.

#i さらに, AI によるタグ付けの精度は未検証であり, 今後の調査が必要である. さらに, 東山動物園の飼育員との意見交換会では,
特定の動物に特化した行動分析と音声解析の導入の提案があり, システムを適切な大きさで分割する必要があるとわかった.

#i 今後は, 動画の切り出しのアルゴリズムとして, 画像の類似度ハッシュと物体検出を組み合わせ, 十分な解析速度を有するシステムの開発を目指す. AI
がタグ付けを行う際に必要十分なデータとして, 複数のカメラ映像, 音声データを加え, , 析の質を向上させ, タグ付けの精度を高める研究をする.

= 引用文献
#{
  set text(size: 6.7pt)
  bibliography("refs.yml", title: [])
}

#pagebreak()

#let handed = (
  //
  "machida": (
    // 自分が担当したところ
    handled: [
      - Squasher, Analyzer の 設計, 実装
      - 研究環境とソースリポジトリの管理
    ],
    // 自分が担当したところの結果
    handledResult: [
      - Squasher
        - Core\
          映像の読み込みと類似度ハッシュと算出, 映像の切り抜きを行い, 算出された値のグラフをリアルタイムに描画するソフトウェア
        - CLI\
          SharePoint にビデオクリップをアップロードするための OAuth 2.0 に準拠したコマンドラインツール
        - Inspector\
          映像の切り抜き精度とタグ付け精度の検証と類似度ハッシュの種類による精度の変化の対照実験するためのツール
      - Analyzer\
        SharePoint との接続とラッパー API の実装, Bard のプロンプトの考案
    ],
    // 自分が担当したところの今後の課題・考察
    futureIssues: [
      - 切り抜きアルゴリズムの改善\
        #i 類似度ハッシュの変化率を推移を, 過去の変化率をもとに指数移動平均を求めているが, 切り抜き範囲が短すぎるという課題があった.\
        #i 切り抜きをするかどうかの動的なしきい値を遅延評価をによって算出し, あとからグループ化することで適切な粒度で切り抜き範囲を決定することができると考察する.
    ],
  ),
  //
  "osakada": (
    // 自分が担当したところ
    handled: [
      - Webアプリケーションの制作
      - WebアプリケーションのUI設計
      - ポスターの制作
    ],
    // 自分が担当したところの結果
    handledResult: [
      - Webアプリケーションを作成することで撮影された動画や, AIでの説明を簡単に閲覧することが可能となった.
    ],
    // 自分が担当したところの今後の課題・考察
    futureIssues: [
      - 検索機能, タグごとでのフィルター機能などのUX向上
      - レスポンシブに対応したUIの設計
      - 動画再生の補助機能
    ],
  ),
  "suzuki": (
    // 自分が担当したところ
    handled: [
      - 研究のロゴ・アイコン作成
      - FigmaでのWEBデザイン
      - 文章作成
    ],
    // 自分が担当したところの結果
    handledResult: [
      #i Figma を使用しての Web デザインは今までやったことのないことだったため, わからないことが多かったが, 配色決めやスクロール画面の作成など,
      自分が想像していなかったような経験ができた.\
      #i 文章作成については, 論文などの論説文の書き方を知らなかったのが原因となり, 完成するのに時間がかかった.
      ロゴ・アイコン作成については、学校の実習の授業で培った技術を活用した.
    ],
    // 自分が担当したところの今後の課題・考察
    futureIssues: [
      #i グラフィック・デザイン技術の向上, 論説文を書けるようにすることを目標に, デザインを見たり勉強したりして, 自分の引き出しを増やすこと. また,
      論文をたくさん読み, 文章のパターンを学び, 論説文を書けるようにする.
    ],
  ),
)

#{
  set text(size: 8pt)
  set par(leading: 1em)

  for p in member.students {
    let hd = handed.at(p.id)
    personHandled(
      //
      person: p,
      handled: hd.handled,
      handledResult: hd.handledResult,
      futureIssues: hd.futureIssues,
    )
    if p.name != member.students.last().name {
      hr
    }
  }
}