# 論文

## 論文題目名

- AI による動画短縮のための場面抽出分類システム開発

## 本文

### はじめに

近年、映像技術の発達に伴い、誰でも長時間の動画を撮影できるようになった。しかし、長時間の動画の場合、重要な場面を見返すことは効率的ではない。効率的に見返せるようにするために自動的に場面を抽出できるシステムを開発した。抽出した動画に説明と事前に私たちがつけたタグ[^1]をつけて、 Web アプリ上に表示する。今回は動物を対象に設定した。理由は動物の場合、動いている部分のみを抽出することで彼らが何を考えて行動しているのか、またその行動にどのような意味があるのか。動物について理解を深められると考えた。この研究を応用して、中学生までの子供を対象に動物園の事前学習に応用に利用でき、動物行動の研究に繋がるだけでなく、幅広い分野の分析・分類やペットの安全を確認するカメラとしての活用、必要な部分のみを動画として保存するため、容量の節約なども期待できると考える。

[メモ](東山動植物園で実際に働く職員の方々にアドバイスをいただいた。実際に働く人の意見を聞くことで自分たちが想定していなかった部分も知ることができた。)

### 目的

動物園に行って動いていなかった動物の動きをアプリ上で閲覧可能にすることで、対象の動物が日頃どのような動きをしているのか、
またその動きにどのような理由があるのかを来場者や体の不自由な人が Web アプリを通して知ってもらうこと。
動物園の職員さんの業務の中に「動物の檻の中で撮影した動画を見て観察記録を書く」というものがあるらしく、2–3 時間必要だと聞いたため、業務の負担を軽減すること、  
中学生までの子供を対象に動物園の事前学習への応用、動物の行動についての研究や天文学などの異なる分野の分析・分類に活用することを最終的な目的としている。

### 研究・活動の内容（概要）

動物園の動物は夜行性も多く、動いていないことが多い。来場者が見たい行動を起こすタイミングは未知であるため、辛抱強く待ったが結局来場者が見たかった行動を見ることができず動物園を後にしてしまう可能性がある。普段どのような行動をどのぐらいの時間帯にしているのかを映像で確認できれば来場者が見たい行動をしている時間に観に行き、その姿を観察することで動物園を回る効率が上がり、問題を解消できるのではないか。
このような想いから動物園の動物の動画を常時撮影し、動いている部分だけをリアルタイムで抽出し 閲覧できる Web アプリを開発した。

研究の手順としては

1. 動物の動きをカメラモジュールで撮影し, それをオブジェクトストレージに保存
2. ビデオクリップを細分化
3. 動物行動を AI で分析
4. 解析したデータを Web アプリ上に表示

という手順を踏んで動いている。
システムは、撮影に Raspberry Pi、動画分析には Bard の Image Input API を利用した。 長時間の動画から自動的に活動シーンだけを切り抜き、AI を用いて分析・分類しアプリとして表示する。
分類した映像に事前に私たちが設定したタグに分類し、抽出した動画が何を写しているのか一目でわかるようになっている。 AI を用いて行動についての説明も追加し、学習にも役立つアプリとした。

### 研究・活動の目的を果たすための調査・活動方法（計画）

<img src="./assets/img/1.png" width="600" title="図1: 行動変化の例" />

リアルタイムで動画を細分化するには、カメラで長時間の動画を撮影する必要がある。しかし、長時間の動画を AI で解析させるのはリソースが限られているため技術的に難しい。そのためまず必要な動きのある場面のみを抽出することにした。ここに 3 枚の行動の画像がある。 1 から 2 枚目の変化は微量だが、 2 から 3 枚目は行動が変化している。
動物行動の変化の境目を検出するため、 今回は 2 つのトリガーをハイブリッド方式で適用した。

<img src="./assets/img/2.png" width="600" title="図2: 類似度ハッシュの変化率を用いた行動変化の境界検出" />

1 つ目は類似度ハッシュを利用して検出する方法。類似度ハッシュは、 画像の特徴を抽出してハッシュ値に変換する関数であり、
同じ画像であれば値は一致し、 違う画像であれば輝度や色、 輪郭の変化によって少しずつ変化する。s0、s1 のように画像が変化した場合も検出できる。
隣り合う画像のハッシュの差を取れば、s1、s2 の間に大きな変化があることが確認できる。
しかし、 s1、s2 のように画像の背景のみが変化した場合でもハッシュが変化してしまうため、
2 つ目に最新の物体検出モデル YOLO v8 を使用し、 類似度ハッシュと合わせて観察対象のみの動きを捉えることにした。
しかし、 YOLO v8 では処理が遅すぎて動画の書き出しに影響が出たため、 今回は類似度ハッシュのみをトリガーとした。

<img src="./assets/img/3.png" width="600" title="図3: Squasher Core のフレーム毎の処理" />

この図はプログラムの設計を表している。レンダラーとモデルが事前に定義したインターフェースを継承して実装した。

<img src="./assets/img/4.png" width="600" title="図4: Squasher Core のアーキテクチャ" />

図から、 レンダラーもモデルもイベントループを継承している。
レンダラー抽象クラスは、初期化処理と画面描画、 1 フレームごとの画面更新を行うメソッドを持つ。 またモデル抽象クラスは、 初期化処理と 1 フレームごとの更新処理と終了処理を行うメソッドを持つ。
組み立てやすくするためにモデルの定義はモデル抽象クラスを継承することで、 モデルを拡張機能のように扱うことができるため分離性が高まる。 これによって複雑な処理を内容によって分離でき、 パッケージ化できるようにした。

また、 イベントループ抽象クラスのコンストラクタにシングルトンストアを注入することで、 継承しているレンダラーやモデル内で簡単にシングルトンストアを使用可能になり、 モデル内で安全にステートの読み書きができるようにした。これによって後続のモデルにデータを渡しやすくなった。
これらの設計の工夫によって、 リアルタイム性に優れた OpenCV や PyQtGraph, PySide などの適切なフレームワークを組み合わせて表示できた。

動画の切り抜き範囲を決めるアルゴリズム
1 フレームごとに類似度ハッシュ算出される。 類似度ハッシュの変化を知るために過去 1 秒の類似度ハッシュの値を最小二乗法　最小二乗法（または、最小自乗法）とは、誤差を伴う測定値の処理において、その誤差の二乗の和を最小にすることで、最も確からしい関係式を求める方法。で近似し、 その傾きを求める。
開発当初は決めたしきい値で切り抜きを行っていたが、 動画によって最適なしきい値が異なるため、 過去の傾きから指数移動平均を取って動的な切り抜きを始める方法に変更した。

動画のタイル化・圧縮を行った。 AI のインプット用にビデオクリップから時間ごとのタイル画像を生成する。これにより、 画像入力ができる Bard が解析可能になる。

ビデオクリップとタイル画像、メタデータをひとまとめにしたファイルをオブジェクトストレージである SharePoint に保存する。
SharePoint は、 Graph API という API エンドポイント経由でアクセスが可能。OAuth 2.0 に準拠するように実装したため、 ユーザーが squasher や viewer にアクセス権限の委任をすることで、 離れたところでも安全にデータを保存が可能になっている。

その後、 AI に行動分析をさせ、 Bard の Image Input API を使用し、 タイル画像の特徴と説明、 事前に私たちで決めたタグ付けを行う。
これらの結果は、 先ほどのメタデータに再格納することにより、 Web アプリで閲覧できるようになる。

viewer では lapsquash ファイルを読み込み、動画を閲覧可能。 また、AI が生成した説明やタグを一緒に確認することにより、動物が何をしているのか、また、その行動の意味が分かりやすくなっている。

<img src="./assets/img/5.png" width="600" title="図5: Viewer の外観" />

viewer のデザインについて。 Material Design を一部採用し、 ニューモフィズムと丸みを持たせたデザインにすることによって操作しやすく、 親しみやすいアプリにした。 また、インタラクティブな UI にすることでパソコン操作が苦手な人でも直感的な操作が可能になっている。
一画面に squasher で得たデータを実際の映像と同時に確認することで動物がなにをしているのかや行動の意味を確認しやすくなっている。

Material Design を採用することで、デバイスの画面サイズに合わせてデザインを最適化しやすいように装飾を減らし、シンプルなデザインを基本としている。そのため、別のデバイスで使用する時でも操作が容易となっている。それに加え ニューモフィズム デザインを採用することで Material Design のシンプルさに加え UX の向上を図った。ニューモフィズムとは Material Design のようなフラットデザインに凹凸を加えたデザインのことである。これを採用することで操作性の向上や他のアプリとの UI での差別化をした。
Web サイトで閲覧するため、画面サイズによってレスポンシブに UI が変化するように設計した。配色には Material Design の Color System を採用することでコンテンツ数が増えた際でも視認性を保つことができた。
実際に動物園の職員の方に画面を見せた際、「見やすい」と、評価を貰った。

<img src="./assets/img/6.png" width="600" title="図6: Viewer の画面構成" />

次に画面の構成について。 この画面では、動物ごとのダイジェストを確認することが可能となっている。
右側には動画を時系列に並べ、 選択することでその動画を閲覧できる。 左下側では動画と説明やタグを確認することが可能である。
一画面に情報が集約されて見ることができるため、他の動画との比較を簡単に行うことが可能になっている。

以上のように、 今回のシステムは大きく分けて 3 つの部分から構成されている。
また、 今学期にチームで分かりやすく全体図を制作した。

squasher は Raspberry Pi 上で動いており、 viewer は好きな デバイス で動く。
しかし、 analyzer はエッジ関数として、 世界中のどこからでも実行できるようになっている。
工夫点としては, エッジ関数としての analyzer のコードが git submodule として, 他の squasher や viewer のコードに組み込まれていることだ。 analyzer の API を tRPC で記述することによって, 他のどちらからでも型安全に呼び出すことができる。

本研究では、 長時間撮影された動物の動画を使用し、 AI 技術を用いて分析・分類し、 必要な動画のみを確認できるシステムを開発する。また、 分析結果のダイジェストは、 アプリケーションを介して誰でも閲覧できるようにすることで、 動物園の課題解決にも取り組む。

### 調査・活動の実施内容と成果

研究期間中は、 カメラモジュールからの動画を Raspberry Pi を使用して対象の動物が動いている場面のみを抽出、細分化して圧縮し、 アップロードを行い、 動画を Bard の Image Input API で分析する。また、 分析結果と共にストレージへ保存し、 lapsquash アプリで AI が生成した説明の文章とともに表示される。
10 月 22 日に東山動物園の飼育員さんに話を聞いた際、動物行動の定義付けに関する難しさを指摘いただいた。どんな動きをしたら"採食"なのか、餌を探しているとき、咀嚼中は"採食"みなされるのか。"喧嘩"と"遊び"、"睡眠"と"座る"の区別をどのように行うのか。

飼育員さんと来場者は根本の視点が異なるため、それぞれニーズも異なる。飼育員は映像のもっと詳細な情報を。一方来場者は一般的な動物の可愛らしい部分を必要としている。

現在の目的だと、"飼育員さんの業務の効率化"とずれてしまっている。
飼育員さんが必要としているのは１つのカメラでは全ての視野を補うのは難しいので、複数のカメラの映像を分析・分類すること。映像のズーム、ズームアウト機能。
対象の動物によって見たい行動や、目的が変わってくるので、目的ごとにアプリを分けて、それを使用者が選択できる方式にすればいい、という意見をいただいた。
動物行動の研究などでは必ずしも「動いている場面」を見たいわけではなく、用途によっては「動いていない場面」を切り抜く場合がある。
小さな対象に対しての切り抜き精度の向上、音声の分析・分類機能。(前述した"喧嘩"と"遊び"、"睡眠"と"座る"の区別をするため。行動によって声の出し方が異なる。)
1 つの動物で複数の定点カメラがある場合を想定していなかった。
動物によって必要なソースや映像以外のもの、必要なカメラの数などがかなり異なってくるため、
１つのアプリケーションで複数の動物っていうのはかなり難しい。解析にも課題がある。
長い中で、これをピックアップするような作業っていうのがそもそも必要はない。
過去に撮りためたビデオデータをその中から行動を切り抜く場合はこのままでも使うことができるが、リアルタイムだとまだ足りない部分がある。
担当者にしか見せない姿もあるが、逆に担当者には見せない姿もあったりする。0,,,.,00.そういうのを知ることができるから思わぬ発見もできる。
勝手に動物に餌をやる人の検出など監視システムとしての利用方法
動物の体温や気温の目を向けると、サーモカメラに対応させなければならない。
その動物にしか聞こえないような音域や、超音波、紫外線、赤外線などの人間には検知できないようなものもあるのでそれにも対応することができれば動物だけでなく、昆虫にも使えて新たな発見があるかも。
魚類などの常に動いている動物が対象の場合、「動いているか」ではなく、行動の部分のみをピックアップすることができれば水中の動物や止まらない動物にも利用できる。
カメラの台数は増やして欲しい。

しかし、基本的に上から取った映像だけになってしまうため、２つの視点で　 AI に学習させれば時間短縮になるらしい。
いろいろ詰め込みすぎた。
１方向の動画のみで判断するのではなく、上からの映像と横からの映像と２方向の映像を使うとより精度が上がる。
上からの動画のみでは起きているのか、寝ているのか区別がつかない場合が多い。

来場者目線なら昨日までの 1 週間の平均で、最近はこの時間帯によく動くというのがデータとして分かるなら、来場者は目的の動物が動いている時間だけを観に行けて、動いていない時間は他の動物を見ることができる。動物が行動するタイミングを待たなくても良くなるため、動物園を回る効率が上がる。
また、「その動物のおすすめコース」なども作れるとの意見をいただいた。
飼育員さんの必要なデータはその時の見る目的によって異なる。発情兆候でも動物によって異なる。
1 日の流れの中の何％をその行動に費やしてるかの割合を調べるのが行動観察だと多い。
繁殖に関連するなら、回数になる。発情兆候が 1 日のうちに何回確認できたか。
普段とは異なる行動も抽出できれば、動物の発情の度合いも分かる。
発情関連のホルモンのことが分かれば、行動を AI に分析させて、AI の信頼性が出た後、 AI にライブで解析させて、今回いいかもっていうのがあれば一緒にするっていうのができる。
動物の出産後、担当者でもその様子を覗きにいくのはシビアで、最悪親が「ここは子育ての環境にない」してしまい、育児を放棄してしまう可能性がある。放棄してしまうと人工保育になる。人口で育てた子供よりも親が育てた子供の方が丈夫であるため、育児放棄の可能性を減らすため、カメラでできるだけ沢山の情報を取る必要がある。子供の数、授乳の回数や音などを拾うことができれば、子供のさまざまなリスクは下げることができる。
動物園の環境エンリッチメントの文献は見た方がいい。
「動物の行動」と一口に言っても相当な量があるため、やることは１つに絞った方がいい。
タグ自体は他の動物にも応用できるが、行動の中身は全然違う。
この研究が実用化レベルであるなら導入したい

### 考察・研究・活動についての想定（計画）と結果の比較対照

一致率は　　と低いが、今後分析・分類の精度や速度を向上させていこうと考えている。

### おわりに

動物園の職員の方々の話を聞いた際、画面の動きだけでなく、音の分析・分類ができると動物の観察に役立つと聞いた。音の分析についてはあまり重要性を感じなかったが、動物の鳴き声は種類、性別、年齢時期などで鳴き方や声色が変わるとのこと。
今後研究していく過程で鳴き声の分析・分類や、夜間暗い中での分析、寝ていることの多い動物が寝ているか生きているかの見分けも行なっていく。
現状, 切り抜き範囲が細かすぎるなどの問題や, 実際に AI に説明させてみたところ, 抽象的な説明しかできなかった。  
今後は, 切り抜きのアルゴリズムとプロンプトの改善, タイル画像のフレーム数とそのサイズの調整が必要である。

### 参考文献

[^1]:
    [ニホンザル野生群の日周活動リズム](https://www.jstage.jst.go.jp/article/ase1911/79/2/79_2_128/_pdf)
    [ニホンザルが毛づくろいをする理由](https://www.osaruland.jp/tips/1820/)
    [ニホンザルの毛づくろい](https://jinrui.zool.kyoto-u.ac.jp/Arashiyama/guide.html)
    [東北野生動物保護管理センター](https://www.env.go.jp/nature/choju/effort/effort5/effort5-3b/4_0130_saru.pdf)

- [ニホンザルってどんな動物？](https://jinrui.zool.kyoto-u.ac.jp/Arashiyama/whatisJmacaque.html)
- [ニホンザルの分布](https://jinrui.zool.kyoto-u.ac.jp/FuscataHome/bunpu.html#:~:text=%E6%97%A5%E6%9C%AC%E3%81%AB%E7%94%9F%E6%81%AF%E3%81%99%E3%82%8B%E3%82%B5%E3%83%AB,%E5%BA%83%E3%81%8F%E5%88%86%E5%B8%83%E3%81%97%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82)
- [睡眠](https://jigokudani-yaenkoen.co.jp/contents/detail?id=117) -[毛ずくろいしらみを....](http://repository.tufs.ac.jp/bitstream/10108/89011/1/field-16_p18-19.pdf)
- ニホンザルの基礎生態と特定計画策定・運用のポイントについて  
  ref: [東北野生動物保護管理センター](https://www.env.go.jp/nature/choju/effort/effort5/effort5-3b/4_0130_saru.pdf)
- チンパンジーとニホンザルの違いについて  
  ref: [絶滅危惧種リスト](https://endangered-species.biz/archives/2457)
- チンパンジーとヒトの共通点・相違点 : 社会的知性を中心に  
  ref: [京都大学学術情報リポジトリ](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/148029/1/100_145.pdf)
- ニホンザルの生態と生息環境について  
  ref: [滋賀県](https://www.pref.shiga.lg.jp/file/attachment/1010347.pdf)
- ニホンザルの定点カメラ映像  
  ref: [YouTube](https://www.youtube.com/live/IjMWzpyCNZM?feature=share)
