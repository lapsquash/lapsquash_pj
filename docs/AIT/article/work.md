# 論文

## 論文題目名

- AI による動画短縮のための場面抽出分類システム開発

## 本文

### はじめに

動物園へ行った際、見に行った動物が動いていなくて寂しい思いをした経験があると思う。この問題を解決するためには、 動物の様子を定点カメラ等で長時間撮影し、
 動いている場面を確認する方法があるが、 長時間撮影された動画を全て見ることは困難である。そのため, 長時間の動画を分析・分類をし、 
 動画から特徴的な行動の部分のみを抽出できるアプリを作成しようと考えた。抽出した動画に説明を付けて、 PC アプリ上に表示させること、 
 動物たちが何を考えて行動しているのかの理解を深められると考えた。しかし、手動で分析・分類をするのは現実的ではない。
 そこで小型コンピュータである Raspberrt Pi を使用した。GPT-4 の API を利用し、AI に学習させて分析・分類を行うことにした。

近年、CG や VFX などの映像技術が様々な分野で活用されており、映像の表現力が高まっている。
動物園に行った際、観察している動物が動かず、本来のその動物の生態が不明瞭な場合がある。このような問題を解決するためには長時間動物を撮影し活動している場面を確認する必要があるが、非常に非効率であるため、自動的に場面を抽出できるシステムを開発した。この研究を応用して、中学生までの子供を対象に動物園の事前学習に応用できたり動物の行動にのみならず、幅広い分野の分析、分類に活用できる。
。

### 目的

動物園に行って動いていなかった動物の動きをアプリ上で閲覧できるようにすること。
対象の動物が日頃どのような動きをしているのか、またその動きにどのような理由があるのかを知ってもらうこと。
このアプリを使うことで、中学生までの子供を対象に動物園の事前学習への応用、動物の行動についての研究、天体など異なる分野の分析、分類に活用することを目標としている。

### 研究・活動の内容（概要）

動物園の動物は夜行性も多く、動いていないことが多い。普段どのような行動をしているのか映像で確認できれば子どもたちも喜ぶのではないか。
このような想いから動物園の動物の動画を常時撮影し、動いている部分だけを取り出し、活動している様子を観察できるようなシステムを開発した。
長時間の動画から自動的に活動シーンだけを切り抜き、AI を用いて分析・分類しアプリとして表示した。
分類した映像にも AI を用いて行動についての説明を生成し、学習にも役立つアプリとした。

### 研究・活動の目的を果たすための調査・活動方法（計画）　　（s０、s１、s２の「この図はプログラムの設計を....のところ」画像貼る）

リアルタイムで動画を細分化するには、ただし、 長時間の動画を AI で解析させるのは、 リソースが限られているので技術的に難しいため、まず必要な動きのある場面だけを抽出することにした。3 枚の行動の画像がある。 1 から 2 枚目の変化は微量だが, 2 から 3 枚目は行動が変化している。
動物行動の変化の境目を検出するため、 今回は 2 つのトリガーをハイブリッド方式で適用した。

1 つは類似度ハッシュを利用して検出する方法。類似度ハッシュは、 画像の特徴を抽出してそれをハッシュ値に変換する関数であり、 
同じ画像であれば値は一致し、 違う画像であれば輝度や色、 輪郭の変化によって少しずつ変化する。s0、s1のように画像が変化した場合も検出できる。
隣り合う画像のハッシュの差を取れば、s1、s2　の間に大きな変化があることが確認できる。
しかし、 s1、s2　のように画像の背景のみが変化した場合でもハッシュが変化してしまうため、
2 つ目に最新の物体検出モデル YOLO v8 を使用し、 類似度ハッシュと合わせて観察対象のみの動きを捉えることにした。
しかし、 YOLO v8 では処理が遅すぎて動画の書き出しに影響が出たため、 今回は類似度ハッシュのみをトリガーとした。

この図はプログラムの設計を表している。レンダラーとモデルが事前に定義したインターフェースを継承して実装した。
図から、 レンダラーもモデルもイベントループを継承している。
レンダラー抽象クラスは、　初期化処理と画面描画、 1 フレームごとの画面更新を行うメソッドを持つ。 またモデル抽象クラスは, 初期化処理と 1 フレームごとの更新処理と終了処理を行うメソッドを持つ
組み立てやすくするためにモデルの定義はモデル抽象クラスを継承することで、 モデルを拡張機能のように扱うことができるので分離性が高まる。 これによって複雑な処理を内容によって分離でき, パッケージ化できるようにした。

また, イベントループ抽象クラスのコンストラクタにシングルトンストアを注入することで、 継承しているレンダラーやモデル内で 簡単にシングルトンストアを使用可能になり、 モデル内で安全にステートの読み書きができるようにしました。これによって後続のモデルにデータを渡しやすくした。
これらの設計の工夫によって、 リアルタイム性に優れた OpenCV や PyQtGraph, PySide などの適切なフレームワークを組み合わせて表示できました.
動画の切り抜き範囲を決めるアルゴリズム
1 フレームごとに類似度ハッシュ算出される。 類似度ハッシュの変化を知るために過去 1 秒の類似度ハッシュの値を最小二乗法で近似し、 その傾きを求める。　
開発当初は決めたしきい値で切り抜きを行っていたが、 動画によって最適なしきい値が異なるため、 過去の傾きから指数移動平均を取って動的な切り抜きを始めるしきい値を決定した。

動画のタイル化・圧縮を行った。 AI のインプット用にビデオクリップから時間ごとのタイル画像を生成する。これにより, 画像入力ができる Bard が解析可能に。

これら(分析したデータ？)のファイルをオブジェクトストレージである SharePoint に保存する。
SharePoint は、 Graph API という API エンドポイント経由でアクセスが可能。OAuth 2.0 に準拠するように実装したため、 ユーザーが squasher や viewer にアクセス権限の委任をすることで、 離れたところでも安全にデータを保存が可能。

その後, AI に行動分析をさせ, Bard の Image Input API を使用し、 タイル画像の特徴と説明、 事前に私たちで決めたタグ付けを AI で行う。
これらの結果は, 先ほどのメタデータに再格納することにより、 PC アプリで閲覧できるようになる。

最後に, 行動の解析結果とダイジェストを PC アプリで閲覧可能ようにした。 squasher で生成した動画を説明やタグと一緒に閲覧が可能。
viewer では lapsquash ファイルを読み込み、動画を閲覧可能。 また、AI が生成した説明やタグを一緒に確認することにより、動物が何をしているのかが分かりやすくなっている。

viewer のデザインについて。 Material Design を一部採用し, ニューモフィズムと丸みを持たせたデザインにすることによって操作しやすく, 親しみやすいアプリにした。 また、インタラクティブな UI にすることでパソコン操作が苦手な人でも直感的な操作が容易になっている。 
一画面に squasher で得たデータを実際の映像と同時に確認することで動物がなにをしているのかや行動の意味を確認しやすくなっている。

次に画面の構成についてです。 この画面ではプロジェクトを一覧表示でき、 動物ごとのダイジェストを確認することが可能。
プロジェクトを開くとこのような画面になっている。 右側には動画を時系列に並べ, 選択することでその動画を閲覧できる。 左側では動画と説明やタグを確認することが可能。

実際のアプリの動作です。 プロジェクトを開くと、 閲覧画面に移行し, squasher が切り抜いた動画を閲覧出来ます。 右側で切り抜いたシーンを選択することが可能で、自分が見たい場面のみを閲覧できる。

以上のように、 今回のシステムは大きく分けて 3 つの部分から構成されている。
また、 今学期にチームで分かりやすく全体図を制作しました。



本研究では、 長時間撮影された動物の動画を使用し、 AI 技術を用いて分析・分類し、 必要な動画のみを確認できるシステムを開発する。また、 分析結果のダイジェストは、 アプリケーションを介して誰でも閲覧できるようにすることで、 動物園の課題解決にも取り組む。

### 調査・活動の実施内容と成果

研究期間中は、 カメラモジュールからの動画を細分化して圧縮し、 アップロードを行い、 動画を GPT-4 の API で分析する。また、 分析結果と共にストレージへ保存し、 アプリケーションで閲覧できるようにする。

### 考察・研究・活動についての想定（計画）と結果の比較対照

### おわりに

x
