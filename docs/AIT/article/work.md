# 論文

## 論文題目名

- AI による動画短縮のための場面抽出分類システム開発

## 本文

### はじめに

映像技術の進歩により、誰でも長時間の動画を撮影できるようになった。しかし、長時間の動画を見直す際に、効率よく重要な場面を探すことは困難である。そこで、重要な場面を自動的に抽出し、予め設定しておいたタグをそれらの動画に付与して Web アプリ上で表示するシステムを開発した。 今回、このシステムは動物の行動を対象として設定した。動物の行動の一部分だけを抽出することで、その行動の出現頻度や種類の分析が容易になり、その動物の行動目的の推察や、行動の意味の理解を深めることにつながる。この研究では、中学生以下の子どもたちが動物園の事前学習するためのツールとして、システムを利用することを想定している。また、このシステムは動物の行動研究に貢献するだけでなく、幅広い分野での動画の分析・分類が可能である。例えば、監視カメラの映像から異常な行動や事件を自動的に検出するなどの利用方法が考えられる。さらに、このシステムは必要な部分だけを動画として保存することから、データ容量を節約でき、比較的安価な機材での構築が可能となっている。

### 目的

### 研究・活動の内容（概要）

動物園の動物は夜行性も多く、動いていないことが多い。来場者が見たい行動を起こすタイミングは未知であるため、辛抱強く待ったが結局来場者が見たかった行動を見ることができず動物園を後にしてしまう可能性がある。普段どのような行動をどのぐらいの時間帯にしているのかを映像で確認できれば来場者が見たい行動をしている時間に観に行き、その姿を観察することで動物園を回る効率が上がり、問題を解消できるのではないか。
このような想いから動物園の動物の動画を常時撮影し、動いている部分だけをリアルタイムで抽出し閲覧できる Web アプリを開発した。研究の手順としては、
・ 動物の動きをカメラモジュールで撮影し、それをオブジェクトストレージに保存
・ ビデオクリップを細分化
・動物行動を AI で分析
・ 解析したデータを Web アプリ上に表示
という手順を踏んで動いている。システムは、撮影に Raspberry Pi、動画分析には Bard の Image Input API を利用した。 長時間の動画から自動的に活動シーンだけを切り抜き、AI を用いて分析・分類しアプリとして表示する。
分類した映像に事前に私たちが設定したタグに分類し、抽出した動画が何を写しているのか一目でわかるようになっている。 AI を用いて行動についての説明も追加し、学習にも役立つアプリとした。

### 研究・活動の目的を果たすための調査・活動方法（計画）

<img src="./assets/img/1.png" width="600" title="図1: 行動変化の例" />

リアルタイムで動画を細分化するには、カメラで長時間の動画を撮影する必要がある。しかし、長時間の動画を AI で解析させるのはリソースが限られているため技術的に難しい。そのためまず必要な動きのある場面のみを抽出することにした。ここに 3 枚の行動の画像がある。 1 から 2 枚目の変化は微量だが、 2 から 3 枚目は行動が変化している。
動物行動の変化の境目を検出するため、 今回は 2 つのトリガーをハイブリッド方式で適用した。

<img src="./assets/img/2.png" width="600" title="図2: 類似度ハッシュの変化率を用いた行動変化の境界検出" />

1 つ目は類似度ハッシュを利用して検出する方法である。類似度ハッシュは、 画像の特徴を抽出してハッシュ値に変換する関数であり、
同じ画像であれば値は一致し、 違う画像であれば輝度や色、 輪郭の変化によって少しずつ変化する。図 3 の s0、s1 のように画像が変化した場合も検出できる。
隣り合う画像のハッシュの差を取れば、s1、s2 の間に大きな変化があることが確認できる。
しかし、 s1、s2 のように画像の背景のみが変化した場合でもハッシュが変化してしまうため、
2 つ目に最新の物体検出モデル YOLO v8 を使用し、 類似度ハッシュと合わせて観察対象のみの動きを捉えることにした。
しかし、 YOLO v8 では処理が遅すぎて動画の書き出しに影響が出たため、 今回は類似度ハッシュのみをトリガーとした。

<img src="./assets/img/3.png" width="600" title="図3: Squasher Core のフレーム毎の処理" />

この図はプログラムの設計を表している。レンダラーとモデルが事前に定義したインターフェースを継承して実装した。

<img src="./assets/img/4.png" width="600" title="図4: Squasher Core のアーキテクチャ" />

図 4 から、 レンダラーもモデルもイベントループを継承している。
レンダラー抽象クラスは、初期化処理と画面描画、 1 フレームごとの画面更新を行うメソッドを持つ。 またモデル抽象クラスは、 初期化処理と 1 フレームごとの更新処理と終了処理を行うメソッドを持つ。
組み立てやすくするためにモデルの定義はモデル抽象クラスを継承することで、 モデルを拡張機能のように扱うことができるため分離性が高まる。 これによって複雑な処理を内容によって分離でき、 パッケージ化できるようにした。

また、 イベントループ抽象クラスのコンストラクタにシングルトンストアを注入することで、 継承しているレンダラーやモデル内で簡単にシングルトンストアを使用可能になり、 モデル内で安全にステートの読み書きができるようにした。これによって後続のモデルにデータを渡しやすくなった。
これらの設計の工夫によって、 リアルタイム性に優れた OpenCV や PyQtGraph, PySide などの適切なフレームワークを組み合わせて表示できた。

動画の切り抜き範囲を決めるアルゴリズム
1 フレームごとに類似度ハッシュが算出される。 類似度ハッシュの変化を知るために過去 1 秒の類似度ハッシュの値を最小二乗法で 1 次式で近似し、その傾きを求める。
開発当初は静的なしきい値で切り抜きを行っていたが、動画によって最適なしきい値が異なるため、過去の傾きから指数移動平均を取って動的なしきい値を使った切り抜きを行う方法に変更した。

動画のタイル化・圧縮を行った。 AI のインプット用にビデオクリップから時間ごとのタイル画像を生成する。これにより、 画像入力ができる Bard で解析可能になる。

ビデオクリップとタイル画像、メタデータをひとまとめにしたファイルをオブジェクトストレージである SharePoint に保存する。
SharePoint は、 Graph API という API エンドポイント経由でアクセスが可能である。OAuth 2.0 に準拠するように実装したため、ユーザーが squasher や viewer にアクセス権限の委任をすることで、 離れたところでも安全にデータを保存が可能になっている。

その後、 AI に行動分析をさせ、 Bard の Image Input API を使用し、 タイル画像の特徴と説明、 事前に私たちで決めたタグ付けを行う。
これらの結果は、 先ほどのメタデータに再格納することにより、 Web アプリで閲覧できるようになる。

viewer では lapsquash ファイルを読み込み、動画を閲覧可能である。 また、AI が生成した説明やタグを一緒に確認することにより、動物が何をしているのか、また、その行動の意味が分かりやすくなっている。

<img src="./assets/img/5.png" width="600" title="図5: Viewer の外観" />

viewer のデザインについて。 Material Design を一部採用し、 ニューモフィズムと丸みを持たせたデザインにすることによって操作しやすく、 親しみやすいアプリにした。 また、インタラクティブな UI にすることでパソコン操作が苦手な人でも直感的な操作が可能になっている。
一画面に squasher で得たデータを実際の映像と同時に確認することで動物がなにをしているのかや行動の意味を確認しやすくなっている。

Material Design を採用することで、デバイスの画面サイズに合わせてデザインを最適化しやすいように装飾を減らし、シンプルなデザインを基本としている。そのため、別のデバイスで使用する時でも操作が容易となっている。それに加え ニューモフィズム デザインを採用することで Material Design のシンプルさに加え UX の向上を図った。ニューモフィズムとは Material Design のようなフラットデザインに凹凸を加えたデザインのことである。これを採用することで操作性の向上や他のアプリとの UI での差別化をした。
Web サイトで閲覧するため、画面サイズによってレスポンシブに UI が変化するように設計した。配色には Material Design の Color System を採用することでコンテンツ数が増えた際でも視認性を保つことができた。
実際に動物園の職員の方に画面を見せた際、「見やすい」と、評価を貰った。

<img src="./assets/img/6.png" width="600" title="図6: Viewer の画面構成" />

次に画面の構成について。 この画面では、動物ごとのダイジェストを確認することが可能となっている。
右側には動画を時系列に並べ、 選択することでその動画を閲覧できる。 左下側では動画と説明やタグを確認することが可能である。
一画面に情報が集約されて見ることができるため、他の動画との比較を簡単に行うことが可能になっている。

以上のように、 今回のシステムは大きく分けて 3 つの部分から構成されている。
また、 今学期にチームで分かりやすく全体図を制作した。

squasher は Raspberry Pi 上で動いており、 viewer は好きなデバイスで動く。
しかし、 analyzer はエッジ関数として、 世界中のどこからでも実行できるようになっている。
工夫点は、エッジ関数としての analyzer のコードが git submodule として、他の squasher や viewer のコードに組み込まれていることである。 analyzer の API を tRPC で記述することによって, 他のどちらからでも型安全に呼び出すことができる。

本研究では、 長時間撮影された動物の動画を使用し、 AI 技術を用いて分析・分類し、 必要な動画のみを確認できるシステムを開発する。また、 分析結果のダイジェストは、 アプリケーションを介して誰でも閲覧できるようにすることで、 動物園の課題解決にも取り組む。

### 調査・活動の実施内容と成果

研究期間中は、 カメラモジュールからの動画を Raspberry Pi を使用して対象の動物が動いている場面のみを抽出、細分化して圧縮し、 アップロードを行い、 動画を Bard の Image Input API で分析する。また、 分析結果と共にストレージへ保存し、 lapsquash アプリで AI が生成した説明の文章とともに表示される。
10 月 22 日に東山動物園の飼育員さんに話を聞いた際、動物行動の定義付けに関する難しさを指摘いただいた。どんな動きをしたら「採食」なのか、餌を探しているとき、咀嚼中は「採食」みなされるのか。「喧嘩」と「遊び」、「睡眠」と「座る」の区別をどのように行うのか。

飼育員さんと来場者は根本の視点が異なるため、それぞれニーズも異なる。飼育員は映像のもっと詳細な情報を。一方来場者は一般的な動物の可愛らしい部分を必要としている。

現在の目的だと、"飼育員さんの業務の効率化"とずれてしまっている。
飼育員さんが必要としているのは１つのカメラでは全ての視野を補うのは難しいので、複数のカメラの映像を分析・分類すること。映像のズーム、ズームアウト機能。
対象の動物によって見たい行動や、目的が変わってくるので、目的ごとにアプリを分けて、それを使用者が選択できる方式にすればいい、という意見をいただいた。
動物行動の研究などでは必ずしも「動いている場面」を見たいわけではなく、用途によっては「動いていない場面」を切り抜く場合がある。
小さな対象に対しての切り抜き精度の向上、音声の分析・分類機能。(前述した「喧嘩」と「遊び」、「睡眠」と「座る」の区別をするため。行動によって声の出し方が異なる。)
1 つの動物で複数の定点カメラがある場合を想定していなかった。
動物によって必要なソースや映像以外のもの、必要なカメラの数などがかなり異なってくるため、
1 つのアプリケーションで複数の動物っていうのはかなり難しい。解析にも課題がある。
長い中で、これをピックアップするような作業はそもそも必要ない。
過去に撮りためたビデオデータをその中から行動を切り抜く場合はこのままでも使うことができるが、リアルタイムだとまだ足りない部分がある。
担当者にしか見せない姿もあるが、逆に担当者には見せない姿もある。そういうのを知ることができるから思わぬ発見もできる。
勝手に動物に餌をやる人の検出など監視システムとしての利用方法
動物の体温や気温の目を向けると、サーモカメラに対応させなければならない。
その動物にしか聞こえないような音域や、超音波、紫外線、赤外線などの人間には検知できないようなものもあるのでそれにも対応できれば動物だけでなく、昆虫にも使えて新たな発見があるかも。
魚類などの常に動いている動物が対象の場合、「動いているか」ではなく、行動の部分のみをピックアップできれば水中の動物や止まらない動物にも利用できる。
カメラの台数は増やして欲しい。

しかし、基本的に上から取った映像だけになってしまうため、２つの視点で AI に学習させれば時間短縮になるらしい。
いろいろ詰め込みすぎた。
1 方向の動画のみで判断するのではなく、上からの映像と横からの映像と 2 方向の映像を使うとより精度が上がる。
上からの動画のみでは起きているのか、寝ているのか区別がつかない場合が多い。

来場者目線なら昨日までの 1 週間の平均で、最近はこの時間帯によく動くというのがデータとして分かるなら、来場者は目的の動物が動いている時間だけを観に行けて、動いていない時間は他の動物を見ることができる。動物が行動するタイミングを待たなくても良くなるため、動物園を周る効率が上がる。
また、「その動物のおすすめコース」なども作れるとの意見をいただいた。
飼育員さんの必要なデータはその時の見る目的によって異なる。発情兆候でも動物によって異なる。
1 日の流れの中の何％をその行動に費やしてるかの割合を調べることが行動観察だと多い。
繁殖に関連するなら、回数になる。発情兆候が 1 日のうちに何回確認できたか。
普段とは異なる行動も抽出できれば、動物の発情の度合いも分かる。
発情関連のホルモンのことが分かれば、行動を AI に分析させて、AI の信頼性が出た後、 AI にライブで解析させて、今回いいかもっていうのがあれば一緒にするっていうのができる。
動物の出産後、担当者でもその様子を覗きにいくのはシビアで、最悪親が「ここは子育ての環境にない」してしまい、育児を放棄してしまう可能性がある。放棄してしまうと人工保育になる。人口で育てた子どもよりも親が育てた子どもの方が丈夫であるため、育児放棄の可能性を減らすため、カメラでできるだけ沢山の情報を取る必要がある。子どもの数、授乳の回数や音などを拾うことができれば、子どものさまざまなリスクは下げることができる。
動物園の環境エンリッチメントの文献は見た方がいい。
「動物の行動」と一口に言っても相当な量があるため、やることは１つに絞った方がいい。
タグ自体は他の動物にも応用できるが、行動の中身は全然違う。
この研究が実用化レベルであるなら導入したい。

### 考察・研究・活動についての想定（計画）と結果の比較対照

一致率は　　と低いが、今後分析・分類の精度や速度を向上させていこうと考えている。

### おわりに

動物園の職員の方々の話を聞いた際、画面の動きだけでなく、音の分析・分類ができると動物の観察に役立つと聞いた。音の分析についてはあまり重要性を感じなかったが、動物の鳴き声は種類、性別、年齢時期などで鳴き方や声色が変わるとのこと。
今後研究していく過程で鳴き声の分析・分類や、夜間暗い中での分析、寝ていることの多い動物が寝ているか生きているかの見分けも行なっていく。
現状、切り抜き範囲が細かすぎるなどの問題や、実際に AI に説明させてみたところ、抽象的な説明しかできなかった。  
今後は, 切り抜きのアルゴリズムとプロンプトの改善, タイル画像のフレーム数とそのサイズの調整が必要である。

### 参考文献

[^1]:
    [ニホンザル野生群の日周活動リズム](https://www.jstage.jst.go.jp/article/ase1911/79/2/79_2_128/_pdf)
    [ニホンザルが毛づくろいをする理由](https://www.osaruland.jp/tips/1820/)
    [ニホンザルの毛づくろい](https://jinrui.zool.kyoto-u.ac.jp/Arashiyama/guide.html)
    [東北野生動物保護管理センター](https://www.env.go.jp/nature/choju/effort/effort5/effort5-3b/4_0130_saru.pdf)

- [ニホンザルってどんな動物？](https://jinrui.zool.kyoto-u.ac.jp/Arashiyama/whatisJmacaque.html)
- [ニホンザルの分布](https://jinrui.zool.kyoto-u.ac.jp/FuscataHome/bunpu.html#:~:text=%E6%97%A5%E6%9C%AC%E3%81%AB%E7%94%9F%E6%81%AF%E3%81%99%E3%82%8B%E3%82%B5%E3%83%AB,%E5%BA%83%E3%81%8F%E5%88%86%E5%B8%83%E3%81%97%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82)
- [睡眠](https://jigokudani-yaenkoen.co.jp/contents/detail?id=117) -[毛ずくろいしらみを....](http://repository.tufs.ac.jp/bitstream/10108/89011/1/field-16_p18-19.pdf)
- ニホンザルの基礎生態と特定計画策定・運用のポイントについて  
  ref: [東北野生動物保護管理センター](https://www.env.go.jp/nature/choju/effort/effort5/effort5-3b/4_0130_saru.pdf)
- チンパンジーとニホンザルの違いについて  
  ref: [絶滅危惧種リスト](https://endangered-species.biz/archives/2457)
- チンパンジーとヒトの共通点・相違点 : 社会的知性を中心に  
  ref: [京都大学学術情報リポジトリ](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/148029/1/100_145.pdf)
- ニホンザルの生態と生息環境について  
  ref: [滋賀県](https://www.pref.shiga.lg.jp/file/attachment/1010347.pdf)
- ニホンザルの定点カメラ映像  
  ref: [YouTube](https://www.youtube.com/live/IjMWzpyCNZM?feature=share)
