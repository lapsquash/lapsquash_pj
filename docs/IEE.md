# 電気学会 台本

## はじめ / 0m13s / スズキ

発表を始めます.  
私たちは, **AI による動画短縮のための場面抽出分類システム開発** の研究を行っています.  
まず, 研究の背景です.

## 1. 背景 / / スズキ

★  
動物園に行ったとき, 見に行った動物が動いていなくて寂しい思いをしたことがあると思います.  
このような問題を解決するためには, 動物の様子を撮影し, 動いている映像を確認する方法がありますが, 長時間撮影された動画を全て見ることは難しいです.  
そのため, 長時間の動画を分析・分類をし, 動画から特徴的な行動の部分を抽出しようと考えました.  
抽出がうまくできればその動画に説明を付けて, PC アプリ上に表示させることで, 動物たちが何を考え行動しているのかの理解を深められると考えました.

★  
長時間の動画を行動ごとにダイジェスト化するために…

まず, 動物の動きをカメラモジュールで撮影し, それをオブジェクトストレージに保存します.
次に, ビデオクリップを細分化し, 動物行動を AI で分析させます.  
そして, 解析したデータを PC アプリ上に表示させるというような流れで研究を進めました.

## 2. 概要

### システム名称の決定 / / スズキ

★  
今回開発したシステムは, タイムラプスを 圧縮する つまり squash するという意味で, lapsquash という名称に決定しました.

### システム全体図 / / スズキ

これが, システムの全体図となります.

<!-- TODO: over_all.svg から吹き出しのテキストを取ってくる (手打ち) -->

動物行動の撮影から抽出結果が PC アプリで見られるまでの流れを順番に説明します.

#### 1. 動物の動きを撮影する / 0m22s / スズキ

まず, 動物行動の変化を捉えるために, Raspberry Pi とカメラモジュールを用いて動物の動きを撮影します.  
★  
写真のようにカメラモジュールをリボンケーブルで繋ぎ, Raspberry Pi に接続しました.

<!-- このようにして動物の行動は撮影できそうですが, 解析させる上では複数の問題があります. -->

#### 2. リアルタイムで細分化 / 1m38s -> 1m25s / マチダ

続いて, リアルタイムで動画を細分化していきます.

★  
ただし, 長時間の動画を AI で解析させるのは, リソースが限られているので技術的に難しいため, まずは, 必要な動きのある場面だけを抽出することにしました.  
★  
このように 3 枚の行動の画像があります. 1 から 2 枚目は変化は無さそうですが, 2 から 3 枚目は行動が変化しています.  
動物行動の変化の境目を検出するために, 今回は 2 つのトリガーをハイブリッド方式で適用してみました.

★  
1 つ目は類似度ハッシュを利用して検出する方法です.  
しかし、$s_1$ と $s_2$ のように画像の背景のみが変化した場合でも、ハッシュが変化してしまいますので,  
2 つ目に最新の物体検出モデル YOLO v8 を使用し, 類似度ハッシュと合わせて, 観察対象のみの動きを捉えることにしました.

しかし, YOLO v8 では処理が遅すぎて動画の書き出しに影響が出たため, 今回は類似度ハッシュのみをトリガーとしました.

★  
類似度ハッシュは, 画像の特徴を抽出して, それをハッシュ値に変換する関数です.  
同じ画像であれば値は一致し, 違う画像であれば, 輝度や色, 輪郭の変化によって, 少しずつ変化します.  
例えば、画像 $s_0$ と $s_1$ のように画像が変化した場合も検出できます.  
隣り合う画像のハッシュの差を取れば, $s_1$ と $s_2$ の間に大きな変化があることが分かります.

さてこのようにして, 動物の動きを撮影し, 動きの変化を検出できるようにし, 長時間の動画を細分化でき, AI に解析させやすくしました.

★
この動画は, Python でリアルタイムで切り抜きを行っている様子で, GUI で類似度ハッシュとその変化率のグラフを見られるようにしました.

★  
この図は実際に実装したプログラムの処理内容です.
この中で特に工夫したのは, プログラムの設計と動画の切り抜き範囲を決めるアルゴリズムです.

★  
この図はプログラムの設計を表しています.  
レンダラーとモデルが事前に定義したインターフェースを継承して実装されています.
このインターフェースについて詳しく説明します.

★  
図から, レンダラーもモデルもイベントループを継承しています.  
レンダラー抽象クラスは, 初期化処理と画面描画, 1 フレームごとの画面更新を行うメソッドを持ちます.
またモデル抽象クラスは, 初期化処理と 1 フレームごとの更新処理と終了処理を行うメソッドを持ちます.

★
モデルの定義にはモデル抽象クラスを継承することで, モデルを拡張機能のように扱えるので, 分離性が高く, 組み立てやすくしました.
これによって複雑な処理を内容によって分離でき, パッケージ化できるようにしました.

★  
また, イベントループ抽象クラスのコンストラクタにシングルトンストアを注入することで, 継承しているレンダラーやモデル内で
簡単にシングルトンストアを使用できるようになり, モデル内で安全にステートの読み書きができるようにしました.

これによって後続のモデルにデータを渡しやすく, 拡張性に優れたプログラムを書くことができました.

★  
これらの設計の工夫によって, リアルタイム性に優れた OpenCV や PyQtGraph, PySide などの  
適切なフレームワークを組み合わせて表示することができました.

★  
もうひとつの工夫点は, 動画の切り抜き範囲を決めるアルゴリズムです.  
1 フレームごとに類似度ハッシュ算出されます. 類似度ハッシュの変化を知るために過去 1 秒の類似度ハッシュの値を最小二乗法で近似し, その傾きを取ります.
はじめは, 決めたしきい値で切り抜きを行っていましたが, 動画によって最適なしきい値が異なるため, 過去の傾きから指数移動平均を取って動的な切り抜きをし始めるしきい値を決めました.

#### 3. 動画のタイル化・圧縮 / 0m33s / マチダ

★  
つづいて, 動画のタイル化・圧縮を行いました.
AI のインプット用にビデオクリップから時間ごとのタイル画像を生成します.  
これにより, 画像入力ができる Bard に解析されることができるようになりました.

#### 4. ストレージサーバーへ保存 / 0m29s / マチダ

★  
これらのファイルをストレージサーバーに保存します. 今回はオブジェクトストレージである SharePoint に保存します.  
SharePoint は, Graph API という API エンドポイント経由でアクセスできます.  
OAuth 2.0 に準拠するように実装したので, ユーザーが squasher や viewer にアクセス権限の委任をすることで, 離れたところでも安全にデータを保存できます.

#### 5. AI に行動分析をさせる / 0m24s / マチダ

★  
その後, AI に行動分析をさせ, Bard の Image Input API を使用して, タイル画像の特徴と説明, 事前に私たちで決めたタグ付けを AI にしてもらいます.  
これらの結果は, 先ほどのメタデータに再格納することで, PC アプリで見られるようにします.

#### 6. 閲覧アプリを制作する / 0m15s / オサカダ

★  
最後に, 行動の解析結果とダイジェストを PC アプリで見られるようにしました.

★
squasher で生成した動画を説明やタグと一緒に閲覧できるようになっています。

実際の動作です。
lapsqash ファイルを読み込み、動画を閲覧できます。
また、AIが生成した説明やタグを一緒に確認することで動物が何をしているのかが分かりやすくなっています。

★
次に viewer のデザインについてです。
Material Design を一部採用し、ニューモフィズムと丸みを持たせたデザインにすることによって操作しやすく、親しみやすいアプリになっています。
また、インタラクティブな UI にすることでパソコン操作が苦手な人でも直感的な操作がしやすくなっています。
sqasher で得たデータを実際の映像と同時に確認することで動物がなにをしているのかや行動の意味を確認しやすくなっています。

<!-- スライドの内容:

  1. 機能:
     - lapsq ファイルから読み込んで (ダイジェスト表示) でクリックしたら該当の動画飛ぶ
     - タグが見れたり秒数が見れたりする
     - UI の工夫点 (ニューモフィズム)
  2. 録画したやつ流す -->

以上のように, 今回のシステムは大きく分けて 3 つの部分から構成されています.  
また, 今学期にチームで分かりやすく全体図を制作しました.

### まとめ, 展望 / / オサカダ

このシステムを使うことで長時間の動画を分析・分類をし, 動画から特徴的な行動の部分を検出し,  
動物行動の理解を深めることで入場者に楽しんでもらえるのではないかと思います.  
また, 子どもたちの事前学習にも活用できるのではと考えています.

ただし現状では, 切り抜き範囲が細かすぎるなどの問題があったり, 実際に AI に説明させてみたところ, “水槽の中で泳ぐサル” としか認識できないようでした.  
今後は, 切り抜きのアルゴリズムとプロンプトの改善や, タイル画像のフレーム数とそのサイズの調整が必要です.

今後はさらに精度を向上し, 動物の生態の研究にも活用されるように研究を続けたいと思います.

## おわり / 0m07s / オサカダ

★  
以上で私たちの研究発表を終わります. ご清聴ありがとうございました.

## 質疑チートシート

Q. 結局, 類似度ハッシュだけをトリガーにして大丈夫だったのか？
A. 実際に類似度ハッシュのみで切り抜きをさせたところ, こちらが期待していた結果が得られましたが, スレッディングなどを改善して YOLO v8 を入れて精度を上げていきたいです.
