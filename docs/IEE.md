# 中間発表 台本

中間発表の作業フォルダー ([SharePoint](https://tiny.cc/lapsq-sh/docs/1_midterm))

## はじめ / 0m13s / マチダ

発表を始めます.

私たちは, **AI による動画短縮のための場面抽出分類システム開発** の研究を行っています.

まず, 研究の経緯です.

## 1. 背景

★  
動物園に行ったとき, 見に行った動物が動いていなくて寂しい思いをしたことがあると思います.  
このような問題を解決するためには, 動物の様子を撮影し, 動いている映像を確認する方法がありますが, 長時間撮影された動画を全て見ることは難しいです.  
そのため, 長時間の動画を分析・分類をし, 動画から特徴的な行動の部分を抽出しようと考えました.  
抽出がうまくできればその動画に説明を付けて, PC アプリ上に表示させることで, 動物たちが何を考え行動しているのかの理解を深められると考えました.

★  
長時間の動画を行動ごとにダイジェスト化するために…

まず, 動物の動きをカメラモジュールで撮影し, それをオブジェクトストレージに保存します.
次に, ビデオクリップを細分化し, 動物行動を AI で分析させます.  
そして, 解析したデータを PC アプリ上に表示させるというような流れで研究を進めました.

## 2. 概要

### システム名称の決定

★  
今回開発したシステムは, タイムラプスを 圧縮する つまり squash するという意味で, lapsquash という名称に決定しました.

### システム全体図

これが, システムの全体図となります.

<!-- TODO: over_all.svg から吹き出しのテキストを取ってくる -->

動物行動の撮影から抽出結果が PC アプリで見られるまでの流れを順番に説明します.

#### 1. 動物の動きを撮影する / 0m22s

まず, 動物行動の変化を捉えるために, Raspberry Pi とカメラモジュールを用いて動物の動きを撮影します.  
★  
写真のようにカメラモジュールをリボンケーブルで繋ぎ, Raspberry Pi に接続しました.

<!-- このようにして動物の行動は撮影できそうですが, 解析させる上では複数の問題があります. -->

#### 2. リアルタイムで細分化 / 1m38s -> 1m25s

続いて, リアルタイムで動画を細分化していきます.

★  
ただし, 長時間の動画を AI で解析させるのは, リソースが限られているので技術的に難しいため, まずは, 必要な動きのある場面だけを抽出することにしました.
★  
このように 3 枚の行動の画像があります. 1 から 2 枚目は変化は無さそうですが, 2 から 3 枚目は行動が変化しています.  
動物行動の変化の境目を検出するために, 今回は 2 つのトリガーをハイブリッド方式で適用してみました.

★  
1 つ目は類似度ハッシュを利用して検出する方法です.  
類似度ハッシュは, 画像の特徴を抽出して, それをハッシュ値に変換する関数です.  
同じ画像であれば値は一致し, 違う画像であれば, 輝度や色, 輪郭の変化によって, 少しずつ変化します.  
例えば、画像 $s_0$ と $s_1$ のように画像が変化した場合も検出できます.  
しかし、$s_1$ と $s_2$ のように画像の背景のみが変化した場合でも、ハッシュが変化してしまいます.  
そこで最新の物体検出モデル YOLO v8 を使用し, 類似度ハッシュと合わせて, 観察対象のみの動きを捉えるようにしました.

しかし, YOLO v8 では処理が遅すぎて動画の書き出しに影響が出たため, 今回は類似度ハッシュのみをトリガーとしました.

さてこのようにして, 動物の動きを撮影し, 動きの変化を検出できるようになりました.
これで, 長時間の動画を細分化でき, AI に解析させやすくなりました.

#### 3. 動画のタイル化・圧縮 / 0m33s

★  
そのあと, AI のインプット用にビデオクリップから時間ごとのタイル画像を生成します.  
これは, 現時点で ChatGPT が画像のインプットしかできないためです.  
タイルの大きさや枚数の調整が今後の課題ですね.  
★  
そしたら, ビデオクリップとタイル画像, メタデータをひとまとめに圧縮します.  
このフォーマットは完全に独自のものです. ここでいうメタデータはいわば箱のようなもので, 後々この箱に AI の解析結果が入り, PC アプリで見れるようにするために使います.

#### 4. ストレージサーバーへ保存 / 0m29s

★  
そしたら, この圧縮したファイルをストレージサーバーに保存します. 今回はオブジェクトストレージである SharePoint に保存します. SharePoint は, Graph API という API エンドポイント経由でアクセスできます.  
OAuth 2.0 に準拠するように実装したので, ユーザーが squasher や viewer にアクセス権限の委任をすることで, 離れたところでも安全にデータを保存できます.

#### 5. AI に行動分析をさせる / 0m24s

★  
その後, AI に行動分析をさせます. ChatGPT-4 の Image Input API を使用して, タイル画像の特徴と説明, 事前に私たちで決めたタグ付けを AI にしてもらいます.  
これらの結果は, 先ほどのメタデータに再格納することで, PC アプリで見れるようにします.  
これらがスムーズにできるようにすることが今後の課題ですね.

#### 6. アプリで見れるようにする / 0m15s

★  
最後に, 行動の解析結果とダイジェストを PC アプリで見れるようにします.  
適切なアクセス権限を持ったアカウントでログインすることで, オブジェクトストレージからデータを取得し, ダイジェストなどを見れるようにします.  
画面に写っているのは Figma で制作した UI デザインです.  
このデザイン案通りにニューモーフィズムな UI を実装することが今後の課題ですね.

以上のように, 今回のシステムは大きく分けて 3 つの部分から構成されています.  
また, 今学期にチームで分かりやすく全体図を制作しました.

### 技術スタック / 0m37s

★  
技術スタックは表の通りです.  
squasher は Raspberry Pi 上で動いており, viewer は好きな PC で動きます.  
しかし, analyzer はエッジ関数として, 世界中のどこからでも実行できるようになっています.  
1 つ工夫点は, エッジ関数としての analyzer のコードが git submodule として, 他の squasher や viewer のコードに組み込まれていることです.  
この理由は analyzer の API を tRPC で記述することによって, 他のどちらからでも型安全に呼び出せるようにするためです.

それでは, それぞれのタスクと実装済みのものを見てみましょう.

### それぞれのタスク / 0m09s

★
表の通りです.

(ほどよい沈黙)

...さて, その他にも今学期やったことがあるので紹介します.

### その他 / 0m34s

★  
私たちの研究は, 電気学会 基礎・材料・共通部門大会 に出ることなっています.  
そのため, 出場するために必要なアブストと全体図を制作し, 提出しました.

また, この研究では, それぞれのシステムのソースコードとプロジェクト進行に必要な情報を, 別々の git リポジトリで管理しています.  
さらに, タスク管理として GitHub Projects を使っています.  
これらの情報は, すべて GitHub の Organization である lapsquash に集約されています.

では, 今後の予定を見てみましょう.

## 3. 今後の予定 / 0m19s

★  
おおむね 7 - 8 月中にはシステムの完成を目指しています.  
細分化に必要な しきい値や AI 分析のためのプロンプトの調整に時間が掛かるので, そこに時間を割けるように研究を進めたいです.
またシステム開発と並行してスズキくんが随時, 進捗や実測データをまとめる予定です.

<!-- 展望 -->

この機能があれば, ダイジェスト化された行動に説明を付けて, 動物園の来場者に動物の行動をより理解してもらえるようになると思います.  
また, 今後動物の行動分析の分野でも活用されることが期待できると考えます.

ただし現状では, 切り抜き範囲が細かすぎるなどの問題があったり, 実際に AI に説明させてみたところ, “水槽の中で泳ぐサル” としか認識できないようでした.  
今後は, 切り抜きのアルゴリズムとプロンプトの改善や, タイル画像のフレーム数とそのサイズの調整が必要です.

<!-- 追加する
> これらが達成できたら, 将来的には動物の生態観察や行動分析に活用されるよう, 精度と分類の詳細性を向上させる予定です. -->

## おわり / 0m07s

★  
以上が私たちの研究の中間発表です. ご清聴ありがとうございました.
