# 中間発表 台本

[中間発表の作業フォルダー](https://tiny.cc/lapsq-sh/docs/1_midterm)

## 表紙

発表を始めます.

私たちは, **AI による動画短縮のための場面抽出分類システム開発** を研究を行っています.

## 1. 経緯, 目的

★  
まず, 研究の経緯です.  
動物園に行っても, 動物園の動物が動いていなかったり, どんな

将来的には動物の生態観察や行動分析に活用し、精度と分類の詳細性を向上させる予定です。

## 2. 今学期やったこと

### チーム結成

★  
まず, 私たちはチームの結成をしました.  
バックエンドとフロントを担当する私と, フロントエンドを担当するオサカダくん, 文書の作成や UI デザインなどを担当するスズキくんです. この 3 人で進んでいます.

### システム名称の決定

★  
目的にあったように, タイムラプスを 圧縮する つまり squash するという意味で, lapsquash という名称に決定しました.  
この名称は私が提案し, この爽やかなロゴは, スズキくんが制作してくれました.

### 実装の概要

★  
研究の概要より, より具体的な実装を説明します.

#### 1. 動物の動きを撮影する

まず, 動物行動の変化を捉えるために, Raspberry Pi とカメラモジュールを用いて動物の動きを撮影します.  
図のようにカメラモジュールをリボンケーブルで繋ぎ, Raspberry Pi に接続します.  
実際に接続した様子を見てみましょう.  
★  
こんな感じです. はい.  
このようにして動物の行動は撮影できるっぽそうですが, 解析させる上では, 複数の問題があります.

#### 2. リアルタイムで細分化

★  
まず, 長時間の動画を AI で効率良く解析させるのは技術的に難しいです. リソースが限られていますからね.  
なので, 必要な動きのある場面だけを抽出すれば良くなりそうです.  
★  
このように 3 枚の行動の画像があります. 1 から 2 枚目は変化は無さそうですが, 2 から 3 枚目は行動が変化しています.  
では, 動物行動の変化の境目を検出するには何をトリガーにすれば良いでしょうか…？  
現時点では, 以下の 2 つのトリガーをハイブリッド方式で適用する予定です.

★  
先ほどの画像に 0, 1, 2 と番号を振ってみましょう.  
ここで, これらの画像に **類似度ハッシュ** というハッシュ関数を適用してみます.  
★  
類似度ハッシュは, 画像の特徴を抽出して, それをハッシュ値に変換する関数です.  
同じ画像であれば値は一致し, 違う画像であれば, 輝度や色, 輪郭の変化によって, 少しずつ変化します.  
つまり, 隣り合う画像のハッシュの差を取れば, $s_1$ と $s_2$ の間に大きな変化があることが分かります.  
★  
まとめると, 単位時間で **類似度ハッシュ** を算出することで, ハッシュの変化率から行動変化の境目を検出できそう ということです. しきい値の設定は今後の課題ですね.  
ちなみに, 実際の動画では, 対象の動物が動いているだけでなく, 背景も動いています. そこで…

★  
最新の物体検出モデル YOLO v8 を使用し, 類似度ハッシュと合わせて, 観察対象のみの動きを捉えることにしました. トリガーの調整が今後の課題です.  
さてこのようにして, 動物の動きを撮影し, 動きの変化を検出できるようになりました.  
これで, 長時間の動画を細分化でき, AI に解析させやすくなりました.

#### 3. 動画のタイル化・圧縮

★  
そのあと, AI のインプット用にビデオクリップから時間ごとのタイル画像を生成します.  
これは, 現時点で ChatGPT が画像のインプットしかできないためです.  
タイルの大きさや枚数の調整が今後の課題ですね.  
★  
そしたら, ビデオクリップとタイル画像, メタデータをひとまとめに圧縮します.  
このフォーマットは完全に独自のものです. ここでいうメタデータはいわば箱のようなもので, 後々この箱に AI の解析結果が入り, PC アプリで見れるようにするために使います.

#### 4. ストレージサーバーへ保存

★  
そしたら, この圧縮したファイルをストレージサーバーに保存します. 今回はオブジェクトストレージである SharePoint に保存します. SharePoint は, Graph API という API エンドポイント経由でアクセスできます.  
OAuth 2.0 に準拠するように実装したので, ユーザーが squasher や viewer にアクセス権限の委任をすることで, 離れたところでも安全にデータを保存できます.

#### 5. AI に行動分析をさせる

★  
その後, AI に行動分析をさせます. ChatGPT-4 の Image Input API を使用して, タイル画像の特徴と説明, 事前に私たちで決めたタグ付けを AI にしてもらいます.  
これらの結果は, 先ほどのメタデータに再格納することで, PC アプリで見れるようにします.  
これらがスムーズにできるようにすることが今後の課題ですね.

#### 6. アプリで見れるようにする

★  
最後に, 行動の解析結果とダイジェストを PC アプリで見れるようにします.  
適切なアクセス権限を持ったアカウントでログインすることで, オブジェクトストレージからデータを取得し, ダイジェストなどを見れるようにします.  
画面に写っているのは Figma で制作した UI デザインです.  
このデザイン案通りにニューモーフィズムな UI を実装することが今後の課題ですね.

以上のように, 今回のシステムは大きく分けて 3 つの部分から構成されています.  
また, 今学期にチームで分かりやすく全体図を制作しました.

### システム全体図

★  
これが lapsquash 全体図です.  
3 つの部分でやり取りするファイル形式を最初に独自に決めることで, それぞれを独立して, 並行開発できるようにしました.

次に, それぞれの部分の技術スタックを見てみましょう.

### 技術スタック

★  
技術スタックは表の通りです.  
squasher は Raspberry Pi 上で動いており, viewer は好きな PC で動きます.  
しかし, analyzer はエッジ関数として, 世界中のどこからでも実行できるようになっています.  
1 つ工夫点は, エッジ関数としての analyzer のコードが git submodule として, 他の squasher や viewer のコードに組み込まれていることです.  
この理由は analyzer の API を tRPC で記述することによって, 他のどちらからでも型安全に呼び出せるようにするためです.

それでは, それぞれのタスクと実装済みのものを見てみましょう.

### それぞれのタスク

★
表の通りです.

...さて, その他にも今学期やったことがあるので紹介します.

### その他

★  
私たちの研究は, 電気学会 基礎・材料・共通部門大会 に出ることなっています.  
そのため, 出場するために必要なアブストと全体図を制作し, 提出しました.

また, この研究では, それぞれのシステムのソースコードとプロジェクト進行に必要な情報を, 別々の git リポジトリで管理しています.  
さらに, タスク管理として GitHub Projects を使っています.  
これらの情報は, すべて GitHub の Organization である lapsquash に集約されています.

では, 今後の予定を見てみましょう.

## 3. 今後の予定

★  
おおむね 7 - 8 月中にはシステムの完成を目指しています.  
細分化に必要なしきい値や AI 分析のためのプロンプトの調整に時間が掛かるので, そこに時間を割けるように研究を進めたいです.
またシステム開発と並行してスズキくんが随時, 進捗や実測データをまとめる予定です.

## おわり

★  
以上が私たちの研究の中間発表です. ご清聴ありがとうございました.
